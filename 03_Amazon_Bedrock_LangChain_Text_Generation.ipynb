{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7fyHODMir5R"
   },
   "source": [
    "# テキスト生成\n",
    "\n",
    "テキスト生成では、以下のようなタスクを行うことができます。\n",
    "\n",
    "- 文章生成\n",
    "- 質問応答\n",
    "- 要約\n",
    "- 翻訳\n",
    "- プログラム生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOFR1KJrjEIX"
   },
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5394,
     "status": "ok",
     "timestamp": 1699358596825,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "QPBR74oBibJA",
    "outputId": "447d557c-b384-4fd1-816c-0427fca1229b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.0.331 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.0.331)\n",
      "Requirement already satisfied: boto3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.28.62)\n",
      "Requirement already satisfied: botocore in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.31.62)\n",
      "Requirement already satisfied: awscli in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.29.62)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (0.6.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (3.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (2.31.0)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (0.0.62)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (2.4.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (2.0.21)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (1.33)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (4.0.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (6.0.1)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (1.26.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (8.2.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from boto3) (0.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from botocore) (1.26.17)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from awscli) (0.4.4)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from awscli) (0.16)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (3.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (1.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.331) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.331) (3.4)\n",
      "Requirement already satisfied: exceptiongroup in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.331) (1.1.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (3.20.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.331) (2.4)\n",
      "Requirement already satisfied: pytest-subtests<0.12.0,>=0.11.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langsmith<0.1.0,>=0.0.52->langchain==0.0.331) (0.11.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (3.0.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.331) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.331) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.331) (2.10.1)\n",
      "Requirement already satisfied: pytest>=7.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain==0.0.331) (7.4.3)\n",
      "Requirement already satisfied: iniconfig in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain==0.0.331) (2.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain==0.0.331) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain==0.0.331) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.331) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.331) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "## 利用するベースモデルのライブラリ (AWS SDK (boto3)) も別途インストールする\n",
    "!pip install langchain==0.0.331 boto3 botocore awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDVvkuRpjgx9"
   },
   "source": [
    "## 文章生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1699358602312,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "LdaD7FwPjcrC"
   },
   "outputs": [],
   "source": [
    "## 入力プロンプトの準備\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = '''\\\n",
    "以下のような頭出しで始まる小説の続きを書いてください。\n",
    "\n",
    "{message}\n",
    "'''\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9247,
     "status": "ok",
     "timestamp": 1699358611555,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "a8Q9GUKukA8T",
    "outputId": "0236038b-891e-4973-c489-5275da0d7e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「私は何を<0xE8><0xA9><0xA3>っているのですか」\n",
      "「私の家の中に足が踏み入っています」\n",
      "It was Meros's turn to get angry. \"What are you doing in my home?\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "## Configuration\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"ai21.j2-ultra-v1\",  # AI21 Labs, Jurassic-2 Ultra v1\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7,  ## サンプリング温度: 0..2 で設定。数値が高い(0.8等)と回答のランダム性が増し、低い(0.2等)と回答がより決定論的になる。\n",
    "        \"maxTokens\": 255,  ## 最大トークン長: 入力トークンと出力トークンの合計の最大値。\n",
    "    },\n",
    ")\n",
    "\n",
    "## Run\n",
    "message = 'メロスは激怒した。'\n",
    "prompt_text=prompt.format(message=message)\n",
    "res = llm(prompt_text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvEX6SeM804_"
   },
   "source": [
    "## 質問応答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1699358611555,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "gZYARylO80Dt"
   },
   "outputs": [],
   "source": [
    "## 入力プロンプトの準備\n",
    "template = '''\\\n",
    "あなたは、非常に知的な質問回答ボットです。\n",
    "もし、真実に基づいた質問をした場合、あなたは答えを教えます。\n",
    "ナンセンスな質問、トリッキーな質問、明確な答えの無い質問には「不明です」と答えます。\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "'''\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1307,
     "status": "ok",
     "timestamp": 1699358612840,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "7WXk5i-Yn4AK",
    "outputId": "cc5a995e-4e09-4c78-bded-ac40a3a760c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本人の平均寿命は日本人の死亡統計によれば100歳です。しかし、現在の日本人の平均寿命は77歳で、2050年までには84歳になる予測されています。\n"
     ]
    }
   ],
   "source": [
    "## Configuration\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"ai21.j2-ultra-v1\",  # AI21 Labs, Jurassic-2 Ultra v1\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7,  ## サンプリング温度: 0..2 で設定。数値が高い(0.8等)と回答のランダム性が増し、低い(0.2等)と回答がより決定論的になる。\n",
    "        \"maxTokens\": 255,  ## 最大トークン長: 入力トークンと出力トークンの合計の最大値。\n",
    "    },\n",
    ")\n",
    "\n",
    "## Run\n",
    "question = '日本人の平均寿命は何歳ですか。'\n",
    "prompt_text=prompt.format(question=question)\n",
    "res = llm(prompt_text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2xBCQVg_Z81"
   },
   "source": [
    "## 要約"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1699358612840,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "c7gZjs6j-VEr"
   },
   "outputs": [],
   "source": [
    "## 入力プロンプトの準備\n",
    "template = '''\\\n",
    "以下の文章を、小学3年生向けに、わかりやすく要約してください。\n",
    "\n",
    "{text}\n",
    "'''\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3561,
     "status": "ok",
     "timestamp": 1699358616395,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "Xvm-xfOW_e2r",
    "outputId": "364e145c-b9e8-48bf-ee74-39f7ffbe99e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tale of Genji is a story written in the Heian period, about a noble family. It was written by a lady named Murasaki Shikibu.\n",
      "Murasaki Shikibu was from a low-level noble family. She married Fujiwara Nobutaka, and they had one daughter. However, her husband died three years after they married. To forget her reality, she started writing the story. This story is now called \"The Tale of Genji\".\n",
      "\n",
      "At that time, paper was valuable, and people enjoyed writing it and sharing it with others. It was also a fun way to have a critic. The Tale of Genji was so popular that it was used to choose the teacher for Murasaki Shikibu's daughter, who was from a high noble family.\n",
      "\n",
      "The Tale of Genji was made into pictures 150 years after it was written. There are two picture collections of this story that are national treasures. It is read in many countries and has been translated into 20 languages.\n"
     ]
    }
   ],
   "source": [
    "## Configuration\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"ai21.j2-ultra-v1\",  # AI21 Labs, Jurassic-2 Ultra v1\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7,  ## サンプリング温度: 0..2 で設定。数値が高い(0.8等)と回答のランダム性が増し、低い(0.2等)と回答がより決定論的になる。\n",
    "        \"maxTokens\": 255,  ## 最大トークン長: 入力トークンと出力トークンの合計の最大値。\n",
    "    },\n",
    ")\n",
    "\n",
    "## Run\n",
    "text = '''『源氏物語』（げんじものがたり）は、平安時代中期に成立した日本の長編物語、小説。文献初出は1008年（寛弘五年）。作者の紫式部にとって生涯で唯一の物語作品である[注 1]。主人公の光源氏を通して、恋愛、栄光と没落、政治的欲望と権力闘争など、平安時代の貴族社会を描いた[1]。\n",
    "下級貴族出身の紫式部は、20代後半で藤原宣孝と結婚し一女をもうけたが、結婚後3年ほどで夫と死別し、その現実を忘れるために物語を書き始めた。これが『源氏物語』の始まりともいわれる[2]。当時、紙は貴重で、紙の提供者がいればその都度書き[注 2]、仲間内で批評し合うなどして楽しんでいたが[注 3]、その物語の評判から藤原道長が娘の中宮彰子の家庭教師として紫式部を呼んだ[注 4]。それを機に宮中に上がった紫式部は、宮仕えをしながら藤原道長の支援の下で物語を書き続け[注 5]、54帖からなる『源氏物語』が完成した[1]。\n",
    "なお、源氏物語は文献初出からおよそ150年後の平安時代末期に「源氏物語絵巻」として絵画化された[4]。現存する絵巻物のうち、徳川美術館と五島美術館所蔵のものは国宝となっている。また現在、『源氏物語』は日本のみならず20か国語を超える翻訳を通じて世界各国で読まれている[注 6]。\n",
    "'''  ## 引用: https://ja.wikipedia.org/wiki/%E6%BA%90%E6%B0%8F%E7%89%A9%E8%AA%9E\n",
    "prompt_text=prompt.format(text=text)\n",
    "res = llm(prompt_text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNXKSpdYBnLr"
   },
   "source": [
    "## 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1699358616395,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "ERiRNOI_DWdv"
   },
   "outputs": [],
   "source": [
    "## 入力プロンプトの準備\n",
    "template = '''\\\n",
    "## 以下の英語の文章を、日本語にギャルっぽく翻訳してください。\n",
    "# English:\n",
    "\n",
    "  {english}\n",
    "\n",
    "# 日本語:\n",
    "'''\n",
    "prompt = PromptTemplate(template=template, input_variables=['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1699358616840,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "fAVFxqjONVr8",
    "outputId": "076eac2f-85fd-44ee-f48f-36c2af05f6a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  \n",
      "私はとても幸せです!\n"
     ]
    }
   ],
   "source": [
    "## Configuration\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"ai21.j2-ultra-v1\",  # AI21 Labs, Jurassic-2 Ultra v1\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7,  ## サンプリング温度: 0..2 で設定。数値が高い(0.8等)と回答のランダム性が増し、低い(0.2等)と回答がより決定論的になる。\n",
    "        \"maxTokens\": 255,  ## 最大トークン長: 入力トークンと出力トークンの合計の最大値。\n",
    "    },\n",
    ")\n",
    "\n",
    "## Run\n",
    "english='''\n",
    "I'm so happy!\n",
    "'''\n",
    "prompt_text=prompt.format(english=english)\n",
    "res = llm(prompt_text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IilIeDpNYsJ"
   },
   "source": [
    "### おまけ: ソースコード変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1699358616841,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "0-5PKx_rBlqL"
   },
   "outputs": [],
   "source": [
    "## 入力プロンプトの準備\n",
    "template = '''\\\n",
    "## Pythonの関数をTypeScriptに変換してください。\n",
    "# Python\n",
    "\n",
    "  {function_code}\n",
    "\n",
    "# TypeScript\n",
    "'''\n",
    "prompt = PromptTemplate(template=template, input_variables=['function_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1465,
     "status": "ok",
     "timestamp": 1699358618300,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "dw8XyRVUCvYy",
    "outputId": "923dc017-0a51-4481-85df-f48ed7f799f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "function predict_proba(x: string[]): Float32Array;\n",
      "function predict_proba(x: string): Float32Array;\n",
      "function predict_proba(x: string[]): Float32Array {\n",
      "if (x.length === 0) {\n",
      "return [];\n",
      "}\n",
      "else {\n",
      "return predict_one_probas(x[0]);\n",
      "}\n",
      "}\n",
      "```\n",
      "function predict_proba(x: string[]): Float32Array {\n",
      "    if (x.length === 0) {\n",
      "        return [];\n",
      "    } else {\n",
      "        return predict_one_probas(x[0]);\n",
      "    }\n",
      "}\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "## Configuration\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"ai21.j2-ultra-v1\",  # AI21 Labs, Jurassic-2 Ultra v1\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7,  ## サンプリング温度: 0..2 で設定。数値が高い(0.8等)と回答のランダム性が増し、低い(0.2等)と回答がより決定論的になる。\n",
    "        \"maxTokens\": 255,  ## 最大トークン長: 入力トークンと出力トークンの合計の最大値。\n",
    "    },\n",
    ")\n",
    "\n",
    "## Run\n",
    "function_code='''\n",
    "def predict_proba(X: Iterable[str]):\n",
    "  return np.array([predict_one_probas(tweet) for tweet in X])\n",
    "'''\n",
    "prompt_text=prompt.format(function_code=function_code)\n",
    "res = llm(prompt_text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkzhm5ncEeEA"
   },
   "source": [
    "## プログラム生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1699358618300,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "dAXWYQVFQiLc"
   },
   "outputs": [],
   "source": [
    "## 入力プロンプトの準備\n",
    "template = '''\\\n",
    "## 次の仕様を満たす関数を、Pythonで作成してください。\n",
    "# 仕様\n",
    "{spec}\n",
    "\n",
    "# 関数\n",
    "{function_name}({args}):\n",
    "\n",
    "'''\n",
    "prompt = PromptTemplate(template=template, input_variables=['spec', 'function_name', 'args'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1794,
     "status": "ok",
     "timestamp": 1699358620077,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "Qs5kNaFqFQIS",
    "outputId": "5318a6fa-c808-4bd5-e186-38f9e18763d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "def fizzbuzz(n):\n",
      " if n % 3 == 0 and n % 5 == 0:\n",
      " return \"FizzBuzz\"\n",
      " elif n % 3 == 0:\n",
      " return \"Fizz\"\n",
      " elif n % 5 == 0:\n",
      " return \"Buzz\"\n",
      " else:\n",
      " return str(n)\n",
      "\n",
      "```\n",
      "\n",
      "# 説明\n",
      "この関数は`fizzbuzz(n)`呼び出すと、`n`を受け取り、3と5の倍数である場合があるので、処理しています。\n",
      "3の倍数の場合、文字列\"Fizz\"を返します。\n",
      "5の倍数の場合、文字列\"Buzz\"を返します。\n",
      "両方の倍数の場合、文字列\"FizzBuzz\"を返します。\n",
      "それ以外の場合は、`n`を元に戻しています。このように処理が行われているため、入力として3と5\n"
     ]
    }
   ],
   "source": [
    "## Configuration\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"ai21.j2-ultra-v1\",  # AI21 Labs, Jurassic-2 Ultra v1\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7,  ## サンプリング温度: 0..2 で設定。数値が高い(0.8等)と回答のランダム性が増し、低い(0.2等)と回答がより決定論的になる。\n",
    "        \"maxTokens\": 255,  ## 最大トークン長: 入力トークンと出力トークンの合計の最大値。\n",
    "    },\n",
    ")\n",
    "\n",
    "## Run\n",
    "spec='''- 入力引数に、1以上の正の整数を受け取ります。\n",
    "- 入力引数の値が3の倍数ならば、文字列 \"Fizz\" を返します。\n",
    "- 入力引数の値が5の倍数ならば、文字列 \"Buzz\" を返します。\n",
    "- 入力引数の値が3の倍数かつ5の倍数ならば、文字列 \"FizzBuzz\" を返します。\n",
    "- 入力引数の値が上記以外の場合、入力引数の値をそのまま返します。\n",
    "'''\n",
    "function_name='fizzbuzz'\n",
    "args='n: Ingeter'\n",
    "\n",
    "prompt_text=prompt.format(spec=spec, function_name=function_name, args=args)\n",
    "res = llm(prompt_text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHKxuCPJLH0t"
   },
   "source": [
    "## 他にどんなことができるのか？\n",
    "\n",
    "また、以下のサイトにて、Jurassic-2 UltraとOpenAIの比較表があります。比較で試している項目は、OpenAIのExample項目になっていますので、ご参考ください。\n",
    "\n",
    "- [Jurassic-2 Ultra vs OpenAI 徹底比較！！ | moritalous blog](https://moritalous.pages.dev/dbb3d7326909769f26cc#%E3%82%B5%E3%83%9E%E3%83%AA%E3%83%BC)\n",
    "\n",
    "ちなみに、OpenAIのExampleは以下のサイトです。\n",
    "\n",
    "- [Examples | OpenAI Platform](https://platform.openai.com/examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuoGNqxCMgN1"
   },
   "source": [
    "## LangChain Model I/O\n",
    "\n",
    "LangChainが用意しているモジュール。\n",
    "言語モデルのアプリケーションの中核を担う部分。\n",
    "\n",
    "以下の3つで構成される。\n",
    "\n",
    "- Prompts: モデルへ入力をテンプレート化、動的選択、管理を担う\n",
    "- Language models: 共通インタフェースを通じて、言語モデルを呼び出す\n",
    "- Output parsers: モデルの出力から必要な情報を抽出する\n",
    "\n",
    "![Model I/O](https://python.langchain.com/assets/images/model_io-1f23a36233d7731e93576d6885da2750.jpg)\n",
    "\n",
    "https://python.langchain.com/docs/modules/model_io/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4uWi74AOaIj"
   },
   "source": [
    "## Prompts\n",
    "\n",
    "言語モデルでのプロンプトとは、ユーザがモデルの応答を誘導するために提供する指示や入力のことです。\n",
    "モデルが文脈を理解し、質問に答えたり、文章を完成させたり、会話に参加したりするような、適切で首尾一貫した言語ベースの出力を生成するのに役立ちます。\n",
    "\n",
    "LangChainでは、プロンプトの生成・操作する上で便利なクラスと関数が用意されています。\n",
    "\n",
    "- Prompt templates: パラメータ化された、プロンプト文字列のテンプレート\n",
    "- Example selectors: プロンプトに含む「例」を動的に選択\n",
    "\n",
    "このノートブックでは、主にPrompt templateを使ってプロンプト文字列を整形してきました。\n",
    "Example selectorsでは、例えば以下のような入力と応答の例をプロンプトに含める際に役に立ちます。\n",
    "(いわゆる few-shot prompting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1699358620078,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "kYOqQfymZyas"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "# 反対語を作るフリをした、タスクの例\n",
    "examples = [\n",
    "    {\"input\": \"嬉しい\", \"output\": \"悲しい\"},\n",
    "    {\"input\": \"高い\", \"output\": \"低い\"},\n",
    "    {\"input\": \"元気\", \"output\": \"無気力\"},\n",
    "    {\"input\": \"明るい\", \"output\": \"暗い\"},\n",
    "    {\"input\": \"風が強い\", \"output\": \"風が穏やか\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"入力: {input}\\n出力: {output}\",\n",
    ")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25,\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"各入力の反対語を答えてください。\",\n",
    "    suffix=\"入力: {adjective}\\n出力:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1699358620078,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "ByN-UeRyb7Bc",
    "outputId": "384a774b-a8b4-4f40-fe15-16fdb8215b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各入力の反対語を答えてください。\n",
      "\n",
      "入力: 嬉しい\n",
      "出力: 悲しい\n",
      "\n",
      "入力: 高い\n",
      "出力: 低い\n",
      "\n",
      "入力: 元気\n",
      "出力: 無気力\n",
      "\n",
      "入力: 明るい\n",
      "出力: 暗い\n",
      "\n",
      "入力: 風が強い\n",
      "出力: 風が穏やか\n",
      "\n",
      "入力: 大きい\n",
      "出力:\n"
     ]
    }
   ],
   "source": [
    "## LengthBasedExampleSelectorは、入力の文字列が長くなると、例の数を少なくできる。\n",
    "prompt = dynamic_prompt.format(adjective=\"大きい\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 587,
     "status": "ok",
     "timestamp": 1699358620657,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "wZLfZk_QcMdx",
    "outputId": "7c9691ff-7c8b-4666-e6ef-49ecb9c376a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 小さい\n",
      "1. 悲しい (happy)\n",
      "2. 低い (tall)\n",
      "3. 無気力 (energetic)\n",
      "4. 暗い (bright)\n",
      "5. 穏やか (strong)\n",
      "6. 小さい (big)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "## Configuration\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"ai21.j2-ultra-v1\",  # AI21 Labs, Jurassic-2 Ultra v1\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7,  ## サンプリング温度: 0..2 で設定。数値が高い(0.8等)と回答のランダム性が増し、低い(0.2等)と回答がより決定論的になる。\n",
    "        \"maxTokens\": 255,  ## 最大トークン長: 入力トークンと出力トークンの合計の最大値。\n",
    "    },\n",
    ")\n",
    "\n",
    "## Run\n",
    "res = llm(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSnBTo1djnJ8"
   },
   "source": [
    "Example selectorでは、他にも入力値に最も似ている(コサイン類似度やn-gramオーバーラップスコアなどで判定)例の組み合わせをプロンプトに選択することもできます。\n",
    "\n",
    "詳細はLangChainのページをご覧ください。\n",
    "\n",
    "https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LT0UnLFLnFb"
   },
   "source": [
    "## Language models\n",
    "\n",
    "LangChainは2種類のモデルの統合されたインタフェースを提供しています。\n",
    "\n",
    "- LLMs (Large Language Models)\n",
    "  - 文字列を入力とし、文字列を返すモデル\n",
    "- Chat models\n",
    "  - チャットメッセージ文字列のリストを入力とし、チャットメッセージの文字列を返すモデル\n",
    "\n",
    "### LLM vs Chat model: どっちがいいの？\n",
    "\n",
    "LLMとChat modelは若干の違いがあります。\n",
    "\n",
    "LLMは、文字列を入力し、文字列を返します。チャットモデルは、文字列のリストを入力し、文字列を返します。チャットモデルのベースはLLMですが、チャットでのメッセージのやり取りのために特別なチューニングがなされています。また入力はメッセージ文字列のリストを受け取りますが、それには通常発信者のラベル（「システム」「AI」「人間」など）が付けられます。そして、AIのチャットメッセージとしての文字列を返します。GPT-4やAnthropic社のClaude 2はチャットモデルとして実装されています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qm_DgxJPPco"
   },
   "source": [
    "## Output parsers\n",
    "\n",
    "言語モデルは、出力としてテキスト文字列を返します。ただ、場合によっては文字列ではなく構造化されたデータで出力して欲しい場合があります。この時、Output parsersの出番です。\n",
    "\n",
    "Output parsersは言語モデルの出力を構造化するのに役に立つモジュールです。\n",
    "LangChainでは、以下のモジュールが用意されています。\n",
    "\n",
    "- List parser: カンマ区切りの文字列リスト\n",
    "- Datetime parser: Pythonのdatetimeフォーマット\n",
    "- Enum parser: PythonのEnumフォーマット\n",
    "- Auto-fixing parser: 別のOutput parserをラップしていて、最初のOutput parserが失敗した場合、別のLLMを呼び出してエラーを修正することができる。また、誤ったフォーマットで出力された文字列を、フォーマットされた命令と共に言語モデルに渡し、修正を依頼することもできる。\n",
    "- Pydantic (JSON) parser: JSONフォーマット\n",
    "- Retry parser: 出力文字列のフォーマットがおかしい場合、元のプロンプトを使って単純に再試行し、より良い出力フォーマットを得るためのモジュール\n",
    "- Structured output parser: 複数のフィールドを返したい場合に利用するフォーマット\n",
    "- XML parser: XMLフォーマット\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1553,
     "status": "ok",
     "timestamp": 1699358622206,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "127j64mUHMne",
    "outputId": "4f4ab6ee-be33-4a02-b55d-384caaa044bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力プロンプト:\n",
      "5つのアイスクリームのフレーバーをリスト化してください。\n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n",
      "\n",
      "出力:\n",
      "['vanilla', 'chocolate', 'strawberry', 'mint', 'cookies and cream']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## example: List parser\n",
    "import boto3\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"5つの{subject}をリスト化してください。\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "model = Bedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"ai21.j2-ultra-v1\",  # AI21 Labs, Jurassic-2 Ultra v1\n",
    "    model_kwargs={\n",
    "        \"temperature\":0,  ## サンプリング温度: 0..2 で設定。数値が高い(0.8等)と回答のランダム性が増し、低い(0.2等)と回答がより決定論的になる。\n",
    "        \"maxTokens\": 255,  ## 最大トークン長: 入力トークンと出力トークンの合計の最大値。\n",
    "    },\n",
    ")\n",
    "\n",
    "_input = prompt.format(subject=\"アイスクリームのフレーバー\")\n",
    "output = model(_input)\n",
    "\n",
    "_output = output_parser.parse(output)\n",
    "print(f\"入力プロンプト:\\n{_input}\\n\\n出力:\\n{_output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1699358622206,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "5SqPMEltTfsC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMGRBQgTjNn5/qpN0E2zrml",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
