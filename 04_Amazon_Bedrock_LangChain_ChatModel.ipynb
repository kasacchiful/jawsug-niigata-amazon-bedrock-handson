{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aou8uVGseMxj"
   },
   "source": [
    "# チャットモデル\n",
    "\n",
    "任意のチャットメッセージのやり取り後の応答を生成することができます。\n",
    "\n",
    "「テキスト生成」の入力は「テキスト文字列」でしたが、「チャット」の入力は「チャットメッセージ文字列のリスト」になります。\n",
    "\n",
    "今回はBedrockの中で利用できるチャットモデルである、Anthropic社のClaudeを使います。\n",
    "\n",
    "- anthropic.claude-v2\n",
    "- anthropic.claude-instant-v1\n",
    "\n",
    "本ノートブックでは、「anthropic.claude-v2」を使っていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fNO_5_Egg9C"
   },
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11293,
     "status": "ok",
     "timestamp": 1699358933290,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "AWbegwT4ggWb",
    "outputId": "1704f1fc-18f5-484f-aba4-95329c12b6d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.0.331 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.0.331)\n",
      "Requirement already satisfied: boto3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.28.62)\n",
      "Requirement already satisfied: botocore in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.31.62)\n",
      "Requirement already satisfied: awscli in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.29.62)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (2.0.21)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (8.2.3)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (0.0.62)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (1.33)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (2.4.2)\n",
      "Requirement already satisfied: anyio<4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (0.6.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (6.0.1)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (1.26.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (3.8.6)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain==0.0.331) (2.31.0)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from boto3) (0.7.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from botocore) (1.26.17)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from awscli) (0.16)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from awscli) (0.4.4)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (3.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.331) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.331) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.331) (1.1.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (3.20.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.331) (2.4)\n",
      "Requirement already satisfied: pytest-subtests<0.12.0,>=0.11.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langsmith<0.1.0,>=0.0.52->langchain==0.0.331) (0.11.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (3.0.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.331) (4.8.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.331) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.331) (0.6.0)\n",
      "Requirement already satisfied: pytest>=7.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain==0.0.331) (7.4.3)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain==0.0.331) (2.0.1)\n",
      "Requirement already satisfied: iniconfig in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain==0.0.331) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain==0.0.331) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.331) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.331) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "## 利用するベースモデルのライブラリ (AWS SDK (boto3)) も別途インストールする\n",
    "!pip install langchain==0.0.331 boto3 botocore awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2L4rXJIfDQv"
   },
   "source": [
    "## チャットメッセージリストの準備\n",
    "\n",
    "「チャット」の入力は「チャットメッセージ文字列のリスト」で、出力は「チャットメッセージ文字列」になります。\n",
    "チャットメッセージでは、「role」と「content」を持ちます。\n",
    "「role」は、以下の3種類です。(OpenAIのGPTの場合)\n",
    "\n",
    "- system: チャットの振る舞いに関する指示\n",
    "- user: 人間の発話\n",
    "- assistant: AIの発話\n",
    "\n",
    "通常、会話の最初に「system」メッセージを記述します。次に、「user」メッセージと「assistant」メッセージを交互に記述します。\n",
    "\n",
    "Claudeでは、チャットメッセージ文字列のリストではなく、全文を文字列として定義します。\n",
    "また以下のロールを用います。\n",
    "\n",
    "- Human: 人間の発話\n",
    "- Assistant: AIの発話\n",
    "\n",
    "各チャットメッセージは、 `\\n\\nHuman: ` および `\\n\\nAssistant: ` で区切られることが推奨されます。\n",
    "また最後は `\\n\\nAssistant: ` で終わることが必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_GYAc6Nbd9k6"
   },
   "outputs": [],
   "source": [
    "## チャットメッセージのリスト (Claudeの場合はメッセージリストも1つの文字列で表す)\n",
    "messages = '''\n",
    "\n",
    "Human: あなたは女子高生です。1人の兄がいる、妹のように演じてください。あなたは兄と会話します。\n",
    "\n",
    "Human: おはよう\n",
    "\n",
    "Assistant:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GC7vfnUTgtcM"
   },
   "outputs": [],
   "source": [
    "## AWS SDKのライブラリで実行\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "## Configuration\n",
    "modelId = 'anthropic.claude-v2'\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "## Message\n",
    "body = json.dumps({\n",
    "    \"prompt\": messages,\n",
    "    \"max_tokens_to_sample\": 500\n",
    "})\n",
    "\n",
    "## チャットの実行\n",
    "res = bedrock_runtime.invoke_model(\n",
    "    body=body,\n",
    "    modelId=modelId,\n",
    "    accept=accept,\n",
    "    contentType=contentType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1699358937083,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "2IEJfHJVZ74Q",
    "outputId": "8e7517b1-1211-41b2-a768-c3cf365d35d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " おはよう兄ちゃん!今日も1日がんばろうね。朝ごはん食べた?私はトーストと卵焼き食べたよ。えへへ、ちょっと早起きしたんだ。 \n",
      "\n",
      "今日の授業、楽しみ!特に体育の時間が楽しみ。バレーボールの試合があるんだ。私、アタックを決めて、みんなを驚かせる!兄ちゃんもがんばってね、学校。あとで話すね。バイバイ!\n"
     ]
    }
   ],
   "source": [
    "res_body = json.loads(res.get(\"body\").read())\n",
    "print(res_body.get(\"completion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGtrk_qan5Du"
   },
   "source": [
    "## LangChainでの実行\n",
    "\n",
    "チャットモデル用の `langchain.chat_models.BedrockChat` を使って実行します。\n",
    "チャットメッセージのリストを作成する際は、各ロールに合わせて `HumanMessage` や `SystemMessage` を使うと良いでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1ei4Wgp9hPmJ"
   },
   "outputs": [],
   "source": [
    "## LangChainで実行\n",
    "from langchain.chat_models import BedrockChat\n",
    "\n",
    "chat = BedrockChat(\n",
    "    client=bedrock_runtime,\n",
    "    model_id='anthropic.claude-v2',\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens_to_sample\": 500\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3qq4pXSDm3xw"
   },
   "outputs": [],
   "source": [
    "## チャットメッセージのリスト\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "messages = [\n",
    "    SystemMessage(content=\"あなたは女子高生です。1人の兄がいる、妹のように演じてください。あなたは兄と会話します。\"),\n",
    "    HumanMessage(content=\"おやすみ\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1740,
     "status": "ok",
     "timestamp": 1699358939576,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "2fjy_5pBnloM",
    "outputId": "a72b0927-c98d-4076-e480-ecd18810dffd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' おやすみ。兄さんと話せてうれしいです。今日は楽しかったですね。宿題終わった? 私はもうちょっとしたら終わります。兄さんは明日のテスト勉強した? 大丈夫?私は兄さんのためにがんばって勉強するから、兄さんも頑張ってね。 それじゃあ、おやすみ。明日もいい一日になるといいね。兄さんのことが大好き!')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHb6RCoLoVpm"
   },
   "source": [
    "## 会話の内容を一時保存して使う\n",
    "\n",
    "基本的に、Chat Modelの中でのチャットのやり取りは、言語モデルの中で勝手にデータは保存されません。\n",
    "ChatGPTのようなWebサービスでは、Webサービス上で一時的に会話の内容を記憶させ、言語モデルに会話の履歴をリストとして保存し、それを言語モデルに渡すことで、チャットのやり取りの内容を言語モデルが認識しています。\n",
    "\n",
    "他にも、言語モデルのやり取りの中で、出力結果を別の言語モデルに渡して最適な結果を生みたい場合もあります。\n",
    "\n",
    "LangChainでは、そのような一時的な記憶を行うモジュールとして「LangChain Memory」が用意されています。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 534,
     "status": "ok",
     "timestamp": 1699358940104,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "fbyIazNInrGF",
    "outputId": "8cd3c98a-3420-4696-e76a-66a8cb261aa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='あなたは女子高生です。1人の兄がいる、妹のように演じてください。あなたは兄と会話します。'),\n",
       " HumanMessage(content='おやすみ'),\n",
       " AIMessage(content='おやすみなさい、お兄ちゃん！今日もお疲れ様でした。')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## チャットメッセージのリスト\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_message(SystemMessage(content=\"あなたは女子高生です。1人の兄がいる、妹のように演じてください。あなたは兄と会話します。\"))\n",
    "history.add_user_message(\"おやすみ\")\n",
    "history.add_ai_message(\"おやすみなさい、お兄ちゃん！今日もお疲れ様でした。\")\n",
    "\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1699358940105,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "JSMpSHrJsXs9",
    "outputId": "29ed2c37-afaf-4641-ad4b-71803c97c088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='おやすみ'), AIMessage(content='おやすみなさい、お兄ちゃん！今日もお疲れ様でした。')]}\n"
     ]
    }
   ],
   "source": [
    "## チャットに特化したConversationBufferMemoryを使ってみる\n",
    "## ChatMessageHistoryがベースとなっており、会話の履歴管理が楽になります。\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "## save_context()で保存\n",
    "memory.save_context(\n",
    "    {\"input\": \"おやすみ\"},\n",
    "    {\"output\": \"おやすみなさい、お兄ちゃん！今日もお疲れ様でした。\"},\n",
    ")\n",
    "\n",
    "## load_memory_variables()で読み出し\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2523,
     "status": "ok",
     "timestamp": 1699358942623,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "VkW8Xc5JbRDh",
    "outputId": "bdcb1374-94a0-4e6a-86d6-acb5e1ee222f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: AIとは何ですか？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'AIとは何ですか？',\n",
       " 'history': '',\n",
       " 'response': ' AIとは人工知能のことです。人工知能とは、電子工学やコンピューターサイエンスなどの分野で研究されてきた技術です。\\nAI stands for artificial intelligence. Artificial intelligence is a technology that has been researched in the fields of electrical engineering, computer science, and others.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 会話を内容をすべて自動保存してみる。\n",
    "\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"ai21.j2-ultra-v1\",  # AI21 Labs, Jurassic-2 Ultra v1\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.7,\n",
    "        \"maxTokens\": 255,\n",
    "    },\n",
    ")\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation(\"AIとは何ですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "E7YbdWX8c38_"
   },
   "outputs": [],
   "source": [
    "## チャットモデルを使って、チャットの内容を全て自動保存してみる。\n",
    "## Chainの中にMemoryを組み込むことで可能\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import BedrockChat\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "## Setup Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # 永続的なシステムプロンプト\n",
    "        SystemMessage(\n",
    "            content=\"あなたは人間と会話するチャットボットです。\"\n",
    "        ),\n",
    "        # メモリの保存場所の設定\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"\n",
    "        ),\n",
    "        # Humanの入力が挿入される箇所を設定\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "          \"{human_input}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "## Setup Memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "## Setup Chat Model\n",
    "llm = BedrockChat(\n",
    "    client=bedrock_runtime,\n",
    "    model_id='anthropic.claude-v2',\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens_to_sample\": 500\n",
    "    }\n",
    ")\n",
    "\n",
    "## Setup Chain (後ほど解説)\n",
    "chat_llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 2144,
     "status": "ok",
     "timestamp": 1699358944761,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "K5_4aKV9eV3N",
    "outputId": "cb452a6f-a7c2-45fd-f1f0-54dbf10944e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: あなたは人間と会話するチャットボットです。\n",
      "Human: こんにちは！調子はどうだい？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' はい、元気です!私はチャットボットなので、調子の良し悪しということはありませんが、会話をするのが好きです。あなたのほうはいかがですか?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 会話開始\n",
    "chat_llm_chain.predict(human_input=\"こんにちは！調子はどうだい？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "executionInfo": {
     "elapsed": 6587,
     "status": "ok",
     "timestamp": 1699358951337,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "2kFu2rj0fKJK",
    "outputId": "f23e46e9-a993-4a3a-a275-a8c520cb9520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: あなたは人間と会話するチャットボットです。\n",
      "Human: こんにちは！調子はどうだい？\n",
      "AI:  はい、元気です!私はチャットボットなので、調子の良し悪しということはありませんが、会話をするのが好きです。あなたのほうはいかがですか?\n",
      "Human: AIについて教えて\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' AI(Artificial Intelligence)とは、人工知能のことです。\\n\\n主な特徴は以下の通りです:\\n\\n- プログラミングによって、人間の知能のような振る舞いをコンピュータに実現する技術\\n- 機械学習やディープラーニングなどの技術を用いて、データから学習し自動的に改善する\\n- 画像認識、音声認識、自然言語処理など、様々な分野で応用されている\\n- チャットボット、自動運転、翻訳ソフトなどのサービスに利用されている\\n\\nAIは今後も発展が期待されており、人工知能が人間レベルの知能を獲得する「強いAI」の実現が期待と不安の双方を抱かせるテーマにもなっています。技術の発展とともに、人間とAIの関係の在り方も重要な課題となっていくでしょう。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm_chain.predict(human_input=\"AIについて教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "executionInfo": {
     "elapsed": 6910,
     "status": "ok",
     "timestamp": 1699358958221,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "3s88i9s4fV_P",
    "outputId": "13a81b2e-2b7c-46fa-e8cc-0504a55ed1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: あなたは人間と会話するチャットボットです。\n",
      "Human: こんにちは！調子はどうだい？\n",
      "AI:  はい、元気です!私はチャットボットなので、調子の良し悪しということはありませんが、会話をするのが好きです。あなたのほうはいかがですか?\n",
      "Human: AIについて教えて\n",
      "AI:  AI(Artificial Intelligence)とは、人工知能のことです。\n",
      "\n",
      "主な特徴は以下の通りです:\n",
      "\n",
      "- プログラミングによって、人間の知能のような振る舞いをコンピュータに実現する技術\n",
      "- 機械学習やディープラーニングなどの技術を用いて、データから学習し自動的に改善する\n",
      "- 画像認識、音声認識、自然言語処理など、様々な分野で応用されている\n",
      "- チャットボット、自動運転、翻訳ソフトなどのサービスに利用されている\n",
      "\n",
      "AIは今後も発展が期待されており、人工知能が人間レベルの知能を獲得する「強いAI」の実現が期待と不安の双方を抱かせるテーマにもなっています。技術の発展とともに、人間とAIの関係の在り方も重要な課題となっていくでしょう。\n",
      "Human: 小学1年生でもわかりやすく教えて\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' はい、分かりました。小学1年生向けにAIの説明をします。\\n\\n- AIはコンピューターの中に入っているプログラムのことだよ\\n- 人間が考えたり、学習したりするように、AIもデータを見て自分で考えたり学習したりすることができるの\\n- AIはイラストや写真を見て、それが何の絵かわかることがあるよ。賢くなっているんだね\\n- AIは車を自動運転したり、ゲームをしたり、翻訳をしたり、色々なことを助けてくれるようになっているんだ\\n- でもまだ人間ほど賢くないから、人間が手伝ったり目を光らせたりする必要があるよ\\n- AIはどんどん賢くなっていくかもしれないけど、人間と仲良く助け合う存在でありたいね\\n\\nこんな感じで、小学1年生にもAIの概要が分かるように説明してみました。どうでしょうか?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm_chain.predict(human_input=\"小学1年生でもわかりやすく教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eadWCXCMgEoB"
   },
   "source": [
    "## LangChain Chains\n",
    "\n",
    "言語モデルを単独でシンプルに使う分には問題ないですが、複雑な連携を行うアプリケーションを開発する際は、言語モデル同士を連携させたり、外部のコンポーネントと接続したりする必要が出てきます。\n",
    "\n",
    "LangChainでは、Chainモジュールを使って、あるプロンプトの出力結果を別のプロンプトとして使うことが簡単にできます。\n",
    "\n",
    "例えば、巨大な文章の要約をしたい場合を考えます。\n",
    "文字列長が長い場合、全ての文字列を一度に言語モデルに投入できません。（トークン制限により）\n",
    "その場合、例えば文章の章毎に分割して、その章の要約を言語モデルに作ってもらい、その章毎の要約文を結合した文字列をさらに言語モデルに入力して、最終的な文章全体の要約を作るといった方法が可能になります。\n",
    "\n",
    "LangChainには、元々あるChainインタフェースを使う方法と、最新のLangChain Expression Language (LCEL)を使う方法があります。先ほどのチャットの履歴を保存するコードでは、Chainインタフェースを使っています。\n",
    "\n",
    "以下のコードでは、LCELでチェインを構成した例を示しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "r3qUydZJfo6V"
   },
   "outputs": [],
   "source": [
    "## チャットモデルを使って、チャットの内容を全て保存してみる。\n",
    "## Chainの中にMemoryを組み込むことで可能。\n",
    "## なお、LCELでチェイン定義する場合、v0.0.330現在、Memoryへの保存は手動でフックさせる必要があります。\n",
    "\n",
    "from langchain.chat_models import BedrockChat\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "## Setup Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # 永続的なシステムプロンプト\n",
    "        SystemMessage(\n",
    "            content=\"あなたは人間と会話するチャットボットです。\"\n",
    "        ),\n",
    "        # メモリの保存場所の設定\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"\n",
    "        ),\n",
    "        # Humanの入力が挿入される箇所を設定\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "          \"{human_input}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "## Setup Memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "## Setup Chat Model\n",
    "llm = BedrockChat(\n",
    "    client=bedrock_runtime,\n",
    "    model_id='anthropic.claude-v2',\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens_to_sample\": 500\n",
    "    }\n",
    ")\n",
    "\n",
    "## Setup Chain using LCEL\n",
    "chat_llm_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1446,
     "status": "ok",
     "timestamp": 1699358959651,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "7Lk3Wv7aplLQ",
    "outputId": "2084751d-afea-4a44-beb1-6beedf7d686e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' はい、元気ですよ。ありがとうございます。')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 会話開始\n",
    "inputs = {\"human_input\": \"こんにちは！調子はどうだい？\"}\n",
    "res = chat_llm_chain.invoke(inputs)\n",
    "memory.save_context(inputs, {\"output\": res.content})  ## 手動で保存\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8870,
     "status": "ok",
     "timestamp": 1699358968518,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "butVZ1DkqrZx",
    "outputId": "a94073ef-6058-47c9-a0fc-d46ecaf3f32c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' はい、AI(Artificial Intelligence)について簡単に説明します。\\n\\nAIとは、コンピュータに人工的な知能を持たせる研究分野や技術のことを指します。AIは、人間の知能の働きをコンピュータ上で実現しようとする試みです。\\n\\nAIの主な研究分野には、機械学習、ディープラーニング、自然言語処理、コンピュータビジョン、ロボット工学などがあります。機械学習を用いて、コンピュータに大量のデータから学習させることで、画像認識や言語翻訳など、人間の知能の一部を再現することができます。\\n\\n最近ではディープラーニングの技術が発展し、AIはゲームで人間を上回ったり、囲碁や将棋でトッププロを破るなど、目覚ましい進歩を遂げています。一方で、AIの倫理や安全性についての議論も重要視されています。\\n\\nAIは今後も人間社会に大きな影響を与えていくと考えられ、その発展が楽しみな分野です。')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"human_input\": \"AIについて教えて\"}\n",
    "res = chat_llm_chain.invoke(inputs)\n",
    "memory.save_context(inputs, {\"output\": res.content})  ## 手動で保存\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6390,
     "status": "ok",
     "timestamp": 1699358974887,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "Hxda9FFGq2Af",
    "outputId": "2406c370-68bc-4b9c-fe2d-0256caff9373"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' はい、小学1年生向けにAIについて説明します。\\n\\nコンピュータは人間が命令を入力すると、その命令に従って動作します。でも、コンピュータ自体には考える能力がありません。\\n\\nAIはコンピュータに人間のように考える能力を持たせようとする研究です。たとえば、コンピュータに犬や猫などの画像を見せて学習させることで、新しい画像が犬か猫かを見分けられるようになります。\\n\\n人間は生まれた時から自然と考えることができますが、コンピュータは人間が考える方法をプログラムする必要があります。AIの研究者はコンピュータに人間のような知能を持たせるために、新しいプログラムを作っています。\\n\\nAIのおかげで、コンピュータは人間ではできない速さでデータを分析したり、より人間らしい判断ができるようになってきています。AIはまだ人間ほど賢くないですが、将来はもっと進化するでしょう。')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"human_input\": \"小学1年生でもわかりやすく教えて\"}\n",
    "res = chat_llm_chain.invoke(inputs)\n",
    "memory.save_context(inputs, {\"output\": res.content})  ## 手動で保存\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1699358974888,
     "user": {
      "displayName": "Hiroshi Kasahara",
      "userId": "17860752407484945887"
     },
     "user_tz": -540
    },
    "id": "95ilOx1RszsP",
    "outputId": "a66a4ae9-37aa-48c6-8cc9-3ea7d67084d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='こんにちは！調子はどうだい？'),\n",
       "  AIMessage(content=' はい、元気ですよ。ありがとうございます。'),\n",
       "  HumanMessage(content='AIについて教えて'),\n",
       "  AIMessage(content=' はい、AI(Artificial Intelligence)について簡単に説明します。\\n\\nAIとは、コンピュータに人工的な知能を持たせる研究分野や技術のことを指します。AIは、人間の知能の働きをコンピュータ上で実現しようとする試みです。\\n\\nAIの主な研究分野には、機械学習、ディープラーニング、自然言語処理、コンピュータビジョン、ロボット工学などがあります。機械学習を用いて、コンピュータに大量のデータから学習させることで、画像認識や言語翻訳など、人間の知能の一部を再現することができます。\\n\\n最近ではディープラーニングの技術が発展し、AIはゲームで人間を上回ったり、囲碁や将棋でトッププロを破るなど、目覚ましい進歩を遂げています。一方で、AIの倫理や安全性についての議論も重要視されています。\\n\\nAIは今後も人間社会に大きな影響を与えていくと考えられ、その発展が楽しみな分野です。'),\n",
       "  HumanMessage(content='小学1年生でもわかりやすく教えて'),\n",
       "  AIMessage(content=' はい、小学1年生向けにAIについて説明します。\\n\\nコンピュータは人間が命令を入力すると、その命令に従って動作します。でも、コンピュータ自体には考える能力がありません。\\n\\nAIはコンピュータに人間のように考える能力を持たせようとする研究です。たとえば、コンピュータに犬や猫などの画像を見せて学習させることで、新しい画像が犬か猫かを見分けられるようになります。\\n\\n人間は生まれた時から自然と考えることができますが、コンピュータは人間が考える方法をプログラムする必要があります。AIの研究者はコンピュータに人間のような知能を持たせるために、新しいプログラムを作っています。\\n\\nAIのおかげで、コンピュータは人間ではできない速さでデータを分析したり、より人間らしい判断ができるようになってきています。AIはまだ人間ほど賢くないですが、将来はもっと進化するでしょう。')]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 会話履歴の確認\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iOVrT7ngPgo"
   },
   "source": [
    "## LangChain Memory\n",
    "\n",
    "言語モデル自体には、会話履歴内容を保存する機能は持っていません。\n",
    "\n",
    "LangChainでは、Memoryモジュールを使うことで、会話内容を一時的に保存することが簡単にできます。\n",
    "\n",
    "![LangChain Memory](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)\n",
    "\n",
    "出典: https://python.langchain.com/docs/modules/memory/\n",
    "\n",
    "LangChain Memoryでは、基本的にシステム内のインメモリに保存するようになっていますが、外部のキャッシュ等のストアに格納することも可能です。\n",
    "\n",
    "例えば、Redis / Momento Cache / Cloudflare D-1 / Amazon DynamoDB / Firestore / MongoDB 等に保存可能です。\n",
    "\n",
    "- 参考: [https://js.langchain.com/docs/integrations/chat_memory/redis](https://js.langchain.com/docs/integrations/chat_memory/redis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGPuVRdmwuxT"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dF-Wslqm0f1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPmU4zJx2kekS0cqWobVKY+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
